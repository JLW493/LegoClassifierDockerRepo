{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/JLW493/LegoClassifierDockerRepo/blob/main/MEM679LegoLearner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xhfOWXXK7dzi",
    "outputId": "5c269e43-31ca-4429-fa8d-7e9016cda647"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected classes: ['64/60601', '64/4162', '64/14769', '64/3958', '64/60592', '64/4070', '64/3008', '64/43093', '64/99207', '64/2540', '64/2436', '64/41677', '64/3002', '64/32123', '64/32523', '64/15573', '64/99206', '64/18674', '64/24866', '64/87079']\n",
      "Dataset split completed!\n",
      "Training data stored in: /content/lego_dataset/train\n",
      "Validation data stored in: /content/lego_dataset/validation\n",
      "Classes: ['10247', '11090', '11211', '11212', '11214', '11458', '11476', '11477', '14704', '14719', '14769', '15068', '15070', '15100', '15379', '15392', '15535', '15573', '15712', '18651', '18654', '18674', '18677', '20482', '22388', '22885', '2357', '2412b', '2420', '24201', '24246', '2429', '2430', '2431', '2432', '2436', '2445', '2450', '2454', '2456', '24866', '25269', '2540', '26047', '2654', '26601', '26603', '26604', '2780', '27925', '28192', '2877', '3001', '3002', '3003', '3004', '3005', '3008', '3009', '3010', '30136', '3020', '3021', '3022', '3023', '3024', '3031', '3032', '3034', '3035', '3037', '30374', '3039', '3040', '30413', '30414', '3062b', '3065', '3068b', '3069b', '3070b', '32000', '32013', '32028', '32054', '32062', '32064', '32073', '32123', '32140', '32184', '32278', '32316', '3245c', '32523', '32524', '32525', '32526', '32607', '32952', '33291', '33909', '34103', '3460', '35480', '3622', '3623', '3660', '3665', '3666', '3673', '3700', '3701', '3705', '3710', '3713', '3749', '3795', '3832', '3937', '3941', '3958', '4032', '40490', '4070', '4073', '4081b', '4085', '4162', '41677', '41740', '41769', '41770', '42003', '4274', '4286', '43093', '43722', '43723', '44728', '4477', '4519', '4589', '4599b', '4740', '47457', '48336', '4865', '48729', '49668', '50950', '51739', '53451', '54200', '59443', '60470', '60474', '60478', '60479', '60481', '60483', '60592', '60601', '6091', '61252', '6134', '61409', '61678', '62462', '63864', '63868', '63965', '64644', '6536', '6541', '6558', '6632', '6636', '85080', '85861', '85984', '87079', '87083', '87087', '87552', '87580', '87620', '87994', '88072', '88323', '92280', '92946', '93273', '98138', '98283', '99206', '99207', '99563', '99780', '99781']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda\n",
      "Epoch 1/20\n",
      "Train Loss: 4.0663, Train Accuracy: 0.1001\n",
      "Val Loss: 2.9172, Val Accuracy: 0.2385\n",
      "Epoch 2/20\n",
      "Train Loss: 2.8934, Train Accuracy: 0.2491\n",
      "Val Loss: 2.2232, Val Accuracy: 0.3812\n",
      "Epoch 3/20\n",
      "Train Loss: 2.3237, Train Accuracy: 0.3570\n",
      "Val Loss: 1.9489, Val Accuracy: 0.4472\n",
      "Epoch 4/20\n",
      "Train Loss: 1.9928, Train Accuracy: 0.4298\n",
      "Val Loss: 1.6504, Val Accuracy: 0.5205\n",
      "Epoch 5/20\n",
      "Train Loss: 1.7385, Train Accuracy: 0.4855\n",
      "Val Loss: 1.6353, Val Accuracy: 0.5298\n",
      "Epoch 6/20\n",
      "Train Loss: 1.5736, Train Accuracy: 0.5291\n",
      "Val Loss: 1.5023, Val Accuracy: 0.5587\n",
      "Epoch 7/20\n",
      "Train Loss: 1.4082, Train Accuracy: 0.5687\n",
      "Val Loss: 1.5617, Val Accuracy: 0.5527\n",
      "Epoch 8/20\n",
      "Train Loss: 1.2631, Train Accuracy: 0.6078\n",
      "Val Loss: 1.5064, Val Accuracy: 0.5817\n",
      "Epoch 9/20\n",
      "Train Loss: 1.1678, Train Accuracy: 0.6394\n",
      "Val Loss: 1.4075, Val Accuracy: 0.5968\n",
      "Epoch 10/20\n",
      "Train Loss: 1.0682, Train Accuracy: 0.6605\n",
      "Val Loss: 1.4649, Val Accuracy: 0.5955\n",
      "Epoch 11/20\n",
      "Train Loss: 0.9838, Train Accuracy: 0.6858\n",
      "Val Loss: 1.4960, Val Accuracy: 0.5840\n",
      "Epoch 12/20\n",
      "Train Loss: 0.8972, Train Accuracy: 0.7087\n",
      "Val Loss: 1.4241, Val Accuracy: 0.6070\n",
      "Epoch 13/20\n",
      "Train Loss: 0.8232, Train Accuracy: 0.7318\n",
      "Val Loss: 1.5112, Val Accuracy: 0.5988\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image  # PIL is the Python Imaging Library, used for opening image files\n",
    "import kagglehub\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import efficientnet_b0\n",
    "from torch.optim import Adam\n",
    "\n",
    "# Downloads dataset\n",
    "# path = 'C:/Users/email/Downloads/Lego Classification DataSet'\n",
    "path = kagglehub.dataset_download(\"ronanpickell/b200c-lego-classification-dataset\")\n",
    "\n",
    "# Gets all folders that have files\n",
    "available_folders = []\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for dir_name in dirs:\n",
    "        available_folders.append(os.path.relpath(os.path.join(root, dir_name), path))\n",
    "\n",
    "# Proportion of validation set\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "#Put in Foldermaking\n",
    "# Path to original dataset (use your dataset's path here)\n",
    "original_dataset_dir = path\n",
    "\n",
    "# Dynamically set the output folder in the current working directory\n",
    "output_dir = os.path.join(os.getcwd(), 'lego_dataset')  # Creates `lego_dataset` in the current script directory\n",
    "\n",
    "# Define training and validation directories\n",
    "train_dir = os.path.join(output_dir, 'train')\n",
    "val_dir = os.path.join(output_dir, 'validation')\n",
    "\n",
    "# Create the train and validation directories\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Selects 20 classes\n",
    "selected_classes = available_folders[1:21]\n",
    "print(f\"Selected classes: {selected_classes}\")\n",
    "\n",
    "\n",
    "# Number of images to use per class\n",
    "MAX_IMAGES_PER_CLASS = 100\n",
    "\n",
    "# Traverse the original dataset directory to split images\n",
    "for root, dirs, files in os.walk(original_dataset_dir):\n",
    "    # If the current folder contains images (e.g., JPG/PNG files)\n",
    "    if any(file.lower().endswith(('.jpg', '.jpeg', '.png')) for file in files):\n",
    "        class_name = os.path.basename(root)  # Use folder name as class label\n",
    "\n",
    "        # Create class-specific subdirectories in train and validation directories\n",
    "        class_train_dir = os.path.join(train_dir, class_name)\n",
    "        class_val_dir = os.path.join(val_dir, class_name)\n",
    "        os.makedirs(class_train_dir, exist_ok=True)\n",
    "        os.makedirs(class_val_dir, exist_ok=True)\n",
    "\n",
    "        # Collect all image paths in the current directory\n",
    "        image_paths = [os.path.join(root, file) for file in files if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        # Shuffle the image list for random selection\n",
    "        random.shuffle(image_paths)\n",
    "\n",
    "        # Limit the number of images to MAX_IMAGES_PER_CLASS\n",
    "        image_paths = image_paths[:MAX_IMAGES_PER_CLASS]\n",
    "\n",
    "        # Compute the split index\n",
    "        split_idx = int(len(image_paths) * (1 - VAL_SPLIT))\n",
    "\n",
    "        # Split the dataset into training and validation sets\n",
    "        train_images = image_paths[:split_idx]\n",
    "        val_images = image_paths[split_idx:]\n",
    "\n",
    "        # Copy training images\n",
    "        for image in train_images:\n",
    "            shutil.copy(image, class_train_dir)\n",
    "\n",
    "        # Copy validation images\n",
    "        for image in val_images:\n",
    "            shutil.copy(image, class_val_dir)\n",
    "\n",
    "print(\"Dataset split completed!\")\n",
    "print(f\"Training data stored in: {train_dir}\")\n",
    "print(f\"Validation data stored in: {val_dir}\")\n",
    "\n",
    "# Define image transformations\n",
    "IMG_SIZE = 64\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),  # Resize to match model input\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Brightness & contrast adjustment\n",
    "    transforms.ToTensor(),                   # Convert image to tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 10\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Class names\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Load a pretrained model\n",
    "model = efficientnet_b0(pretrained=True)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)  # Replace final layer\n",
    "\n",
    "# Freeze the base model layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Extend train_model to return metrics for plotting\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20):\n",
    "    \"\"\"Trains Model\n",
    "\n",
    "    Args:\n",
    "        model: EfficientNet_B0\n",
    "        train_loader: DataLoader from training set\n",
    "        val_loader: DataLoader from validation set\n",
    "        criterion: Cross Entropy Loss\n",
    "        optimizer: Optimizer established above\n",
    "        num_epochs: Number of Epochs\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        train_losses: Losses on training set per epoch\n",
    "        val_losses: Losses on validation set per epoch\n",
    "        train_accuracies: Accuracy on training set per epoch\n",
    "        val_accuracies: Accuracy on validation set per epoch\n",
    "    \"\"\"\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss, train_correct = 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            train_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = train_correct / len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                val_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = val_correct / len(val_loader.dataset)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "# Train the model and collect metrics\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs=20\n",
    ")\n",
    "\n",
    "# Plot performance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"lego_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Select one image from each class and run predictions\n",
    "def predict_images_from_classes(model, class_names, val_dir):\n",
    "    \"\"\"Demonstrates Performance by Predicting Images From Each Class\n",
    "\n",
    "    Args:\n",
    "        model: EfficientNet-B0\n",
    "        class_names (): Names of each class\n",
    "        val_dir: Validation Set Directory\n",
    "    \"\"\"\n",
    "    if not isinstance(model, torch.nn.Module):\n",
    "        raise ValueError(\"The `model` parameter must be a PyTorch model.\")\n",
    "\n",
    "    model.eval()\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    for class_name in class_names:\n",
    "        class_folder = os.path.join(val_dir, class_name)\n",
    "        image_files = [\n",
    "            os.path.join(class_folder, img) for img in os.listdir(class_folder)\n",
    "            if img.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "        ]\n",
    "\n",
    "        if not image_files:\n",
    "            print(f\"No images found in class folder: {class_folder}\")\n",
    "            continue\n",
    "\n",
    "        image_path = random.choice(image_files)\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(image_path).convert(\"RGB\")\n",
    "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "        # Perform prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(input_tensor)\n",
    "            predicted_class_idx = output.argmax(dim=1).item()\n",
    "            predicted_class = class_names[predicted_class_idx]\n",
    "\n",
    "        # Display the image and prediction\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {class_name}, Predicted: {predicted_class}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "# Run predictions on one image per class\n",
    "predict_images_from_classes(model, class_names, val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "T_0cm85G7zvT",
    "outputId": "1203ac93-e80a-4e86-ccaa-36f143c3b1d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub) (4.66.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision matplotlib kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sz_ccjCuS1hV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOibbX+Nf+Iy7OoB3H9rk8q",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
